input {
  kafka {
    bootstrap_servers => "kafka:9092"   # 도커 환경에서는 'kafka'가 적합
    topics => ["product-view-log", "purchase-activity-log"]
    group_id => "elk-logstash"
    codec => plain {   # plain 코덱으로 변경
      charset => "UTF-8"
    }
     client_id => "logstash"
        auto_offset_reset => "earliest"
        consumer_threads => 3
        max_poll_records => "5000"
        max_poll_interval_ms => "300000"
  }
    http {
      port => 5000  # SpringBoot에서 Logstash로 로그를 전송할 때 사용할 포트
    }
}

filter {
  mutate {
    add_field => { "logType" => "kafka-log" }
    rename => { "message" => "logMessage" }
  }

  if [logMessage] =~ "^\{.*\}$" {
    json {
      source => "logMessage"
      target => "parsed_json"
      add_tag => [ "json_processed" ]
      tag_on_failure => [ "_jsonparsefailure_custom" ]
    }

    # JSON 파싱 성공 시 타임스탬프 처리
    if "_jsonparsefailure_custom" not in [tags] {
      date {
        match => ["parsed_json[timestamp]", "ISO8601"]
        target => "@timestamp"
      }
    }
  } else {
    mutate {
      add_tag => [ "_not_json" ]
    }
  }

  # 디버깅을 위한 메타데이터 추가
  mutate {
    add_field => {
      "[@metadata][kafka_topic]" => "%{[topic]}"
      "[@metadata][kafka_partition]" => "%{[partition]}"
      "[@metadata][kafka_offset]" => "%{[offset]}"
    }
  }
}

output {
  elasticsearch {
    hosts => ["https://elasticsearch:9200"]
    user => "elastic"
    password => "Ccenter123456!"
    index => "kafka-logs-%{+YYYY.MM.dd}"
    ssl_enabled => true
    ssl_certificate_authorities => ["/usr/share/logstash/config/elasticsearch.crt"]
    ssl_verification_mode => "none"
  }
}
