input {
  kafka {
    bootstrap_servers => "kafka:9092"   # 도커 환경에서는 'kafka'가 적합
    topics => ["product-view-log", "purchase-activity-log"]
    group_id => "elk-logstash"
    codec => plain {   # plain 코덱으로 변경
      charset => "UTF-8"
    }
  }
  # HTTP input 설정 추가
    http {
      port => 5000  # SpringBoot에서 Logstash로 로그를 전송할 때 사용할 포트
    }
}

filter {
  mutate {
    add_field => { "logType" => "kafka-log" }
    rename => { "message" => "logMessage" }
}

  # JSON 파싱 시도를 하기 전에 조건 확인
  if [logMessage] =~ "^\{.*\}$" {
    json {
      source => "logMessage"
      target => "parsed_json"
      add_tag => [ "json_processed" ]
      tag_on_failure => [ "_jsonparsefailure_custom" ]  # 파싱 실패 시 태그 추가
    }
  } else {
    mutate {
      add_tag => [ "_not_json" ]  # JSON이 아닌 경우 태그 추가
    }
  }
}

output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]   # Elasticsearch의 도커 호스트 이름에 맞춤
        user => "elastic"                 # Elasticsearch 사용자 이름
        password => "Ccenter123456!"       # Elasticsearch 사용자 비밀번호
    index => "kafka-logs-%{+YYYY.MM.dd}"     # 로그가 저장될 인덱스 패턴
  }
  stdout { codec => rubydebug }  # 디버깅을 위한 콘솔 출력
}
